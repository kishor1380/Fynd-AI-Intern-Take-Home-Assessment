
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fynd AI Intern Assessment - Task 1: Rating Prediction via Prompting\n",
    "\n",
    "**Objective:** Design prompts that classify Yelp reviews into 1-5 stars, returning structured JSON.\n",
    "\n",
    "**LLM Used:** Google Gemini API (gemini-1.5-flash)\n",
    "\n",
    "**Dataset:** Yelp Reviews from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q google-generativeai pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configure Gemini API\n",
    "\n",
    "Get your free API key from: https://makersuite.google.com/app/apikey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Gemini API\n",
    "API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"  # Replace with your actual API key\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Initialize the model\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "print(\"Gemini API configured successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Sample Dataset\n",
    "\n",
    "Download the Yelp dataset from: https://www.kaggle.com/datasets/omkarsabnis/yelp-reviews-dataset\n",
    "Place the CSV file in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"yelp.csv\")  # Update filename if different\n",
    "\n",
    "# Check dataset structure\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and prepare data\n",
    "# Assuming columns are 'text' and 'stars' (adjust if different)\n",
    "df_clean = df[['text', 'stars']].dropna()\n",
    "df_clean['stars'] = df_clean['stars'].astype(int)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"\\nStar distribution:\")\n",
    "print(df_clean['stars'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample ~200 rows, stratified by star rating\n",
    "np.random.seed(42)\n",
    "\n",
    "sampled = df_clean.groupby('stars', group_keys=False).apply(\n",
    "    lambda x: x.sample(min(len(x), 40), random_state=42)\n",
    ")\n",
    "\n",
    "if len(sampled) > 200:\n",
    "    sampled = sampled.sample(200, random_state=42)\n",
    "\n",
    "sampled = sampled.reset_index(drop=True)\n",
    "\n",
    "print(f\"Sample size: {len(sampled)}\")\n",
    "print(f\"\\nSample star distribution:\")\n",
    "print(sampled['stars'].value_counts().sort_index())\n",
    "\n",
    "sampled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prompting Approaches\n",
    "\n",
    "I will implement 3 different prompting strategies:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Zero-Shot Naive\n",
    "\n",
    "**Strategy:** Simple, direct instruction with minimal constraints.\n",
    "\n",
    "**Expected Issues:** May produce invalid JSON, inconsistent formatting, or ratings outside 1-5 range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_zero_shot(review: str) -> str:\n",
    "    return f\"\"\"You are an assistant that rates customer reviews.\n",
    "\n",
    "Read the following Yelp review and decide how many stars (1 to 5) the customer is likely to give.\n",
    "\n",
    "Return a JSON object with exactly these keys:\n",
    "- \"predicted_stars\": an integer from 1 to 5\n",
    "- \"explanation\": a brief explanation of your reasoning.\n",
    "\n",
    "Review:\n",
    "\"\"\"{review}\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Structured with Schema\n",
    "\n",
    "**Strategy:** Explicit schema definition, examples, and strict formatting rules.\n",
    "\n",
    "**Improvements:**\n",
    "- Explicit allowed values [1,2,3,4,5]\n",
    "- JSON examples to guide format\n",
    "- Clear \"no extra text\" rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_structured(review: str) -> str:\n",
    "    return f\"\"\"You are an assistant that classifies Yelp reviews into star ratings from 1 to 5.\n",
    "\n",
    "Task:\n",
    "1. Read the review.\n",
    "2. Decide the most likely star rating from this discrete set: [1, 2, 3, 4, 5].\n",
    "3. Return a strict JSON object with exactly these keys:\n",
    "   - \"predicted_stars\": integer, one of 1, 2, 3, 4, or 5\n",
    "   - \"explanation\": short string (max 2 sentences) explaining the rating.\n",
    "\n",
    "Rules:\n",
    "- Do not include any extra keys.\n",
    "- Do not include comments or Markdown.\n",
    "- The response must be valid JSON that can be parsed by a standard JSON parser.\n",
    "\n",
    "Examples of valid responses:\n",
    "{{\"predicted_stars\": 5, \"explanation\": \"Very positive tone and strong praise.\"}}\n",
    "{{\"predicted_stars\": 2, \"explanation\": \"Mostly negative with several complaints.\"}}\n",
    "\n",
    "Now classify this review:\n",
    "\n",
    "Review:\n",
    "\"\"\"{review}\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3: Chain-of-Thought with JSON Constraint\n",
    "\n",
    "**Strategy:** Encourage internal reasoning about sentiment analysis while constraining output to JSON only.\n",
    "\n",
    "**Improvements:**\n",
    "- Step-by-step sentiment analysis guidance\n",
    "- Better handling of nuanced/mixed reviews\n",
    "- Internal reasoning improves consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_cot_constrained(review: str) -> str:\n",
    "    return f\"\"\"You are an expert sentiment analyst for Yelp reviews.\n",
    "\n",
    "First, reason step by step about:\n",
    "- Sentiment polarity (positive/negative/neutral)\n",
    "- Strength of sentiment\n",
    "- Specific positives and negatives mentioned\n",
    "- Whether the user would recommend the place to others\n",
    "\n",
    "Then, after you finish your reasoning, output ONLY a JSON object with no extra text.\n",
    "\n",
    "JSON format (mandatory):\n",
    "{{\n",
    "  \"predicted_stars\": <integer 1-5>,\n",
    "  \"explanation\": \"<one or two short sentences summarizing why this rating was chosen>\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- The JSON must be valid and parseable.\n",
    "- Use your internal reasoning to pick the most likely rating from 1, 2, 3, 4, or 5.\n",
    "- Do not output your intermediate reasoning, only the final JSON.\n",
    "\n",
    "Review:\n",
    "\"\"\"{review}\"\"\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Call Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str, max_retries=3) -> str:\n",
    "    \"\"\"Call Gemini API with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = model.generate_content(prompt)\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                print(f\"API call failed (attempt {attempt+1}/{max_retries}): {e}\")\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "            else:\n",
    "                print(f\"API call failed after {max_retries} attempts: {e}\")\n",
    "                return \"{}\"\n",
    "    return \"{}\"\n",
    "\n",
    "# Test the function\n",
    "test_response = call_llm(\"Say 'Hello, I am working!' in JSON format\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. JSON Parsing Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_parse_json(text: str):\n",
    "    \"\"\"Safely parse JSON from LLM response.\"\"\"\n",
    "    try:\n",
    "        # Try to find first '{' and last '}' to strip extra text\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start == -1 or end == -1:\n",
    "            return None\n",
    "        \n",
    "        snippet = text[start:end+1]\n",
    "        # Remove markdown code blocks if present\n",
    "        snippet = snippet.replace('```json', '').replace('```', '')\n",
    "        \n",
    "        obj = json.loads(snippet)\n",
    "        \n",
    "        # Validate required keys\n",
    "        if \"predicted_stars\" in obj and \"explanation\" in obj:\n",
    "            # Ensure predicted_stars is valid\n",
    "            pred = obj[\"predicted_stars\"]\n",
    "            if isinstance(pred, (int, float)) and 1 <= pred <= 5:\n",
    "                return obj\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prompt(sampled_df, prompt_fn, model_name=\"approach\"):\n",
    "    \"\"\"Evaluate a prompting approach on the sample dataset.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    json_valid = 0\n",
    "    responses = []\n",
    "    \n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    for i, row in sampled_df.iterrows():\n",
    "        if i % 20 == 0:\n",
    "            print(f\"Progress: {i}/{len(sampled_df)}\")\n",
    "        \n",
    "        review = row[\"text\"]\n",
    "        true_star = int(row[\"stars\"])\n",
    "        \n",
    "        # Generate prompt and call LLM\n",
    "        prompt = prompt_fn(review)\n",
    "        raw = call_llm(prompt)\n",
    "        \n",
    "        # Parse response\n",
    "        parsed = safe_parse_json(raw)\n",
    "        \n",
    "        if parsed is not None and isinstance(parsed.get(\"predicted_stars\"), (int, float)):\n",
    "            pred_star = int(parsed[\"predicted_stars\"])\n",
    "            # Clamp to valid range\n",
    "            pred_star = max(1, min(5, pred_star))\n",
    "            json_valid += 1\n",
    "        else:\n",
    "            # Fallback to neutral rating\n",
    "            pred_star = 3\n",
    "        \n",
    "        y_true.append(true_star)\n",
    "        y_pred.append(pred_star)\n",
    "        \n",
    "        responses.append({\n",
    "            \"review\": review[:100] + \"...\" if len(review) > 100 else review,\n",
    "            \"true_stars\": true_star,\n",
    "            \"predicted_stars\": pred_star,\n",
    "            \"raw_response\": raw[:200] + \"...\" if len(raw) > 200 else raw,\n",
    "            \"json_valid\": parsed is not None,\n",
    "            \"explanation\": parsed.get(\"explanation\", \"N/A\") if parsed else \"N/A\"\n",
    "        })\n",
    "        \n",
    "        # Small delay to respect API rate limits\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    json_valid_rate = json_valid / len(sampled_df)\n",
    "    \n",
    "    metrics = {\n",
    "        \"approach\": model_name,\n",
    "        \"accuracy\": round(acc, 4),\n",
    "        \"mae\": round(mae, 4),\n",
    "        \"json_valid_rate\": round(json_valid_rate, 4),\n",
    "        \"num_samples\": len(sampled_df)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{model_name} completed!\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"JSON Valid Rate: {json_valid_rate:.4f}\")\n",
    "    \n",
    "    return metrics, pd.DataFrame(responses), y_true, y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Run Evaluations\n",
    "\n",
    "**Note:** This will make API calls. With 200 samples and 3 approaches = 600 API calls.\n",
    "At 0.5s delay per call, this takes ~5 minutes per approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Approach 1: Zero-Shot\n",
    "metrics_zero, df_zero, y_true_zero, y_pred_zero = evaluate_prompt(\n",
    "    sampled, prompt_zero_shot, \"Zero-Shot Naive\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Approach 2: Structured\n",
    "metrics_struct, df_struct, y_true_struct, y_pred_struct = evaluate_prompt(\n",
    "    sampled, prompt_structured, \"Structured with Schema\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Approach 3: Chain-of-Thought\n",
    "metrics_cot, df_cot, y_true_cot, y_pred_cot = evaluate_prompt(\n",
    "    sampled, prompt_cot_constrained, \"Chain-of-Thought Constrained\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Results Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "metrics_list = [metrics_zero, metrics_struct, metrics_cot]\n",
    "metrics_df = pd.DataFrame(metrics_list)\n",
    "\n",
    "print(\"\\n=== COMPARISON TABLE ===\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(metrics_df[\"approach\"], metrics_df[\"accuracy\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[0].set_title(\"Accuracy Comparison\", fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel(\"Accuracy\")\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# MAE comparison\n",
    "axes[1].bar(metrics_df[\"approach\"], metrics_df[\"mae\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[1].set_title(\"Mean Absolute Error\", fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel(\"MAE (lower is better)\")\n",
    "axes[1].tick_params(axis='x', rotation=15)\n",
    "\n",
    "# JSON validity rate\n",
    "axes[2].bar(metrics_df[\"approach\"], metrics_df[\"json_valid_rate\"], color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "axes[2].set_title(\"JSON Validity Rate\", fontsize=12, fontweight='bold')\n",
    "axes[2].set_ylabel(\"Valid JSON Rate\")\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].tick_params(axis='x', rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"metrics_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices for each approach\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "approaches = [\n",
    "    (\"Zero-Shot Naive\", y_true_zero, y_pred_zero),\n",
    "    (\"Structured with Schema\", y_true_struct, y_pred_struct),\n",
    "    (\"CoT Constrained\", y_true_cot, y_pred_cot)\n",
    "]\n",
    "\n",
    "for idx, (name, y_t, y_p) in enumerate(approaches):\n",
    "    cm = confusion_matrix(y_t, y_p, labels=[1, 2, 3, 4, 5])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], \n",
    "                xticklabels=[1,2,3,4,5], yticklabels=[1,2,3,4,5])\n",
    "    axes[idx].set_title(name, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_ylabel(\"True Stars\")\n",
    "    axes[idx].set_xlabel(\"Predicted Stars\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrices.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some example predictions\n",
    "print(\"\\n=== SAMPLE PREDICTIONS ===\\n\")\n",
    "\n",
    "for i in [0, 50, 100, 150]:\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"Review: {df_zero.iloc[i]['review']}\")\n",
    "    print(f\"True Stars: {df_zero.iloc[i]['true_stars']}\")\n",
    "    print(f\"Zero-Shot Predicted: {df_zero.iloc[i]['predicted_stars']} - {df_zero.iloc[i]['explanation']}\")\n",
    "    print(f\"Structured Predicted: {df_struct.iloc[i]['predicted_stars']} - {df_struct.iloc[i]['explanation']}\")\n",
    "    print(f\"CoT Predicted: {df_cot.iloc[i]['predicted_stars']} - {df_cot.iloc[i]['explanation']}\")\n",
    "    print(\"\\n\" + \"-\"*100 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions to CSV\n",
    "df_zero.to_csv(\"predictions_zero_shot.csv\", index=False)\n",
    "df_struct.to_csv(\"predictions_structured.csv\", index=False)\n",
    "df_cot.to_csv(\"predictions_cot.csv\", index=False)\n",
    "\n",
    "# Save metrics\n",
    "metrics_df.to_csv(\"metrics_comparison.csv\", index=False)\n",
    "\n",
    "print(\"Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Discussion and Analysis\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "#### 1. JSON Validity\n",
    "- **Zero-Shot Naive**: Often includes extra text, markdown formatting, or malformed JSON\n",
    "- **Structured with Schema**: Significant improvement in JSON validity due to explicit examples and constraints\n",
    "- **CoT Constrained**: Best JSON validity as the prompt explicitly separates reasoning from output\n",
    "\n",
    "#### 2. Accuracy and Consistency\n",
    "- **Zero-Shot**: May struggle with nuanced reviews (e.g., mixed sentiments, sarcasm)\n",
    "- **Structured**: More stable predictions due to clear guidelines\n",
    "- **CoT**: Handles borderline cases better (2-3 or 3-4 star reviews) through internal reasoning\n",
    "\n",
    "#### 3. Failure Modes Observed\n",
    "- Long reviews sometimes truncated or partially analyzed\n",
    "- Sarcastic reviews occasionally misclassified\n",
    "- Mixed sentiment reviews benefit most from CoT approach\n",
    "\n",
    "#### 4. Trade-offs\n",
    "- **Speed**: Zero-Shot is fastest, CoT slightly slower due to reasoning\n",
    "- **Reliability**: Structured and CoT are more reliable for production use\n",
    "- **Cost**: All approaches use similar token counts with Gemini\n",
    "\n",
    "### Recommendations:\n",
    "For production deployment, **Structured with Schema** or **CoT Constrained** are recommended based on:\n",
    "- Higher JSON validity rates\n",
    "- Better handling of edge cases\n",
    "- More consistent and predictable outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
